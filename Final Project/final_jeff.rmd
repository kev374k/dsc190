```{r, echo = FALSE}
data <- read.table('data.csv', sep=',', header=TRUE)
```

## Question 2: Correlation Between Experience and Programming Languages Used

```{r, echo = FALSE}
library(ggplot2)
library(reshape2)
library(dplyr)
language_data <- data[!is.na(data[["Q6"]]), ]

# Create experience groups from Q6 (assumes column Q6 exists)
language_data$experience_group <- factor(
  language_data$Q6,
  levels = c("< 1 years", "1-2 years", "3-5 years", "5-10 years", "10-20 years", "20+ years")
)

# Subset relevant language columns (e.g., Q7_Part_1, Q7_Part_2, ...)
language_columns <- grep("^Q7_", names(language_data), value = TRUE)
exclude_languages <- c("Q7_Part_12", "Q7_OTHER")
filtered_language_columns <- setdiff(language_columns, exclude_languages)
# Create a binary matrix of language usage (1 = language used, 0 = not used)
binary_matrix <- as.data.frame(ifelse(language_data[filtered_language_columns] != "", 1, 0))
binary_matrix$experience_group <- language_data$experience_group

# Aggregate counts by experience group and language
language_counts <- aggregate(. ~ experience_group, data = binary_matrix, sum)
language_mapping <- c(
  Q7_Part_1 = "Python",
  Q7_Part_2 = "R",
  Q7_Part_3 = "SQL",
  Q7_Part_4 = "C",
  Q7_Part_5 = "C++",
  Q7_Part_6 = "Java",
  Q7_Part_7 = "JavaScript",
  Q7_Part_8 = "Julia",
  Q7_Part_9 = "Swift",
  Q7_Part_10 = "Bash",
  Q7_Part_11 = "MATLAB"
)
language_counts_long <- melt(language_counts, id.vars = "experience_group", 
                             variable.name = "language", value.name = "count")
language_counts_long$language <- language_mapping[language_counts_long$language]
language_counts_long$language <- factor(
  language_counts_long$language,
  levels = c("Python", "R", "SQL", "C", "C++", "Java", "JavaScript", "Julia", 
             "Swift", "Bash", "MATLAB")
)

# Plot grouped bar chart
ggplot(language_counts_long, aes(x = experience_group, y = count, fill = language)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Programming Languages Used by Experience Group",
    x = "Experience Group",
    y = "Number of Respondents",
    fill = "Programming Language"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

selected_languages <- c("Python", "R", "SQL")
focused_data <- subset(language_counts_long, language %in% selected_languages)

# Calculate proportions within each experience group
focused_data <- focused_data %>%
  group_by(experience_group) %>%
  mutate(proportion = count / sum(count))

# Plot stacked bar chart (relative proportions)
ggplot(focused_data, aes(x = experience_group, y = proportion, fill = language)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = scales::percent) +
  labs(
    title = "Proportion of Python, R, and SQL Usage by Experience Group",
    x = "Experience Group",
    y = "Proportion of Respondents",
    fill = "Programming Language"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Question 4: Hiring Preferences of Larger vs. Smaller Companies

### Methods

To investigate age and salary differences between employees of large and small companies, we began by removing rows with missing salary or company size data from the dataset. In order to have numeric values for variables such as age and salary instead of strings, the midpoints of given ranges were used as data points instead of the entire range.

Next, two two-sample t-tests were conducted. The first two-sample t-test compared average age of employees at large vs. small companies. The second test compared average salary of employees at large vs. small companies. Both tests were two-tailed, meaning we are looking for a difference in either direction. Density plots were generated to visualize the two distributions for each metric right next to each other. Finally, a correlation analysis was conducted to find a confounding relationship between age and salary.

```{r, echo = FALSE}
# Remove rows with missing salary or company data
salary_data <- data[data$Q20 != "" & data$Q24 != "", ]

# Function to calculate midpoints from age ranges
convert_age_to_midpoint <- function(age_range) {
  matches <- regmatches(age_range, gregexpr("[0-9]+", age_range))[[1]]
  if (length(matches) == 2) {
    return((as.numeric(matches[1]) + as.numeric(matches[2])) / 2)
  } else {
    return(80)
  }
}

# Apply the function to Q1 column
salary_data$age_midpoint <- sapply(salary_data$Q1, convert_age_to_midpoint)

# Function to calculate midpoints from salary ranges
convert_salary_to_midpoint <- function(salary_range) {
  clean_string <- gsub("[\\$,]", "", salary_range)  # Remove $ and commas
  matches <- regmatches(clean_string, gregexpr("[0-9]+", clean_string))[[1]]
  if (length(matches) == 2) {
    return((as.numeric(matches[1]) + as.numeric(matches[2])) / 2)
  } else {
    return(700000)
  }
}

# Apply the function to Q24 column
salary_data$salary_midpoint <- sapply(salary_data$Q24, convert_salary_to_midpoint)

# Create binary company size variable
salary_data$company_size <- ifelse(salary_data$Q20 %in% c("0-49 employees", "50-249 employees", "250-999 employees"), "Small", "Large")
salary_data$company_size <- factor(salary_data$company_size, levels = c("Small", "Large"))

# T-test for age
age_test <- t.test(age_midpoint ~ company_size, data = salary_data)
# T-test for salary
salary_test <- t.test(salary_midpoint ~ company_size, data = salary_data)

salary_data$jittered_age <- jitter(salary_data$age_midpoint, amount = 10)
salary_data$jittered_salary <- jitter(salary_data$salary_midpoint, amount = 10000)

# Plot results of T-tests
ggplot(salary_data, aes(x = jittered_age, fill = company_size)) +
  geom_density(alpha = 0.5) +
  labs(
    title = "Distribution of Age by Company Size",
    x = "Age (Midpoint)",
    y = "Density",
    fill = "Company Size"
  ) +
  theme_minimal() +
  annotate(
    "text", x = mean(salary_data$age_midpoint, na.rm = TRUE), y = 0.02,
    label = paste0("t = ", round(age_test$statistic, 2), "\n", 
                   "p = ", round(age_test$p.value, 4)),
    color = "black"
  )

ggplot(salary_data, aes(x = jittered_salary, fill = company_size)) +
  geom_density(alpha = 0.5) +
  labs(
    title = "Distribution of Salary by Company Size",
    x = "Salary (Midpoint)",
    y = "Density",
    fill = "Company Size"
  ) +
  theme_minimal() +
  annotate(
    "text", x = mean(salary_data$salary_midpoint, na.rm = TRUE), y = 0.00001,
    label = paste0("t = ", round(salary_test$statistic, 2), "\n", 
                   "p = ", round(salary_test$p.value, 4)),
    color = "black"
  )

# Correlation analysis between age and salary
cor.test(salary_data$age_midpoint, salary_data$salary_midpoint, method = "pearson")
```

### Analysis

Before discussing the results of the two t-tests conducted, we had to check the assumptions of the two-sample t-test. For both tests, the variances of the two distributions were roughly equal. For the test comparing ages, the two distributions of age were found to be roughly normal, passing the normality assumption for that test. However, the salary distributions were found to deviate slightly from a normal distribution with a right skew. Because our sample size was deemed large (**20,000+**), we continued with both two-sample t-tests.

The two-sample t-test on ages resulted in a t-statistic of -5.15 and p-value of \~0, so we reject the null hypothesis meaning there is a significant difference in mean age between small and large companies. In this case, we observed larger companies having older employees. The two-sample t-test on salaries resulted in a t-statistic of -15.4 and p-value of \~0, so we reject the null hypothesis, meaning there is a significant difference in mean salary between large and small companies. In this case, the observed mean salary of large companies was higher.

The correlative analysis between age and salary resulted in a Pearson's correlation coefficient of **0.266**. This shows a relatively weak positive relationship between age and salary.

### Conclusion

Large companies tend to higher older employees for higher salaries, meaning these employees are likely more experienced. The correlation between age and salary was found to be positive, but relatively weak.

## Advanced Analysis: Predict Career Trajectory of Job-Seekers

### Methods

To investigate the relationship between individual factors like experience and skill with someone's career path within data-related jobs, I conducted a multinomial logistic regression analysis. This model was chosen because we wanted interpretable results that were appropriate for multi-classification.

We had to first clean the data by removing rows where one of the explanatory or response variables were missing. We then converted the programming language columns into one-hot encoded numerical values where **1** means that language is used by the individual. Education and experience were encoded as ordinal numerical values with higher levels of education and experience having higher values. Finally, the model was tested on a train-test split with results plotted on a confusion matrix.

```{r, echo = FALSE}
library(nnet)

regression_data <- data[, c("Q4","Q6","Q7_Part_1","Q7_Part_2","Q7_Part_3","Q7_Part_4","Q7_Part_5","Q7_Part_6","Q5")]

# Remove rows where any of Q1, Q4, Q6, or Q5 has the value ""
regression_data <- regression_data[!(regression_data$Q4 == "I prefer not to answer" | regression_data$Q4 == "" | regression_data$Q6 == "" | regression_data$Q5 == "" | regression_data$Q5 == "Student"), ]
colnames(regression_data) <- c("Q4"="Education", "Q6"="Experience", "Q5"="Title","Q7_Part_1"="Python","Q7_Part_2"="R","Q7_Part_3"="SQL","Q7_Part_4"="C","Q7_Part_5"="C++","Q7_Part_6"="Java")[colnames(regression_data)]

# Convert the programming languages used into numerical values
language_columns <- c("Python","R","SQL","C","C++","Java")
regression_data[language_columns] <- lapply(regression_data[language_columns], function(col) ifelse(col != "", 1, 0))

education_levels <- c(
  "No formal education past high school",
  "Some college/university study without earning a bachelor’s degree",
  "Bachelor’s degree",
  "Master’s degree",
  "Doctoral degree",
  "Professional degree")
experience_levels <- c(
  "I have never written code",
  "< 1 years",
  "1-2 years",
  "3-5 years",
  "5-10 years",
  "10-20 years",
  "20+ years"
)

regression_data$Education <- as.numeric(factor(regression_data$Education, levels = education_levels, ordered = TRUE))
regression_data$Experience <- as.numeric(factor(regression_data$Experience, levels = experience_levels, ordered = TRUE))

regression_data$Title <- factor(regression_data$Title)

set.seed(42)
train_indices <- sample(1:nrow(regression_data), size = 0.75 * nrow(regression_data))
train_data <- regression_data[train_indices, ]
test_data <- regression_data[-train_indices, ]

multi_log_model <- multinom(
  Title ~ Education + Experience + .,
  data = train_data
)

# Predict class labels
predictions <- predict(multi_log_model, newdata = test_data)

# Confusion matrix
confusion_matrix <- table(test_data$Title, predictions)

# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", round(accuracy * 100, 2), "%\n")

```

```{r, echo = FALSE}
# Create the confusion matrix
confusion_matrix <- table(test_data$Title, predictions)

# Convert the confusion matrix to a data frame for visualization
confusion_df <- as.data.frame(as.table(confusion_matrix))
colnames(confusion_df) <- c("Actual", "Predicted", "Count")

# Plot the confusion matrix as a heatmap
ggplot(confusion_df, aes(x = Predicted, y = Actual, fill = Count)) +
  geom_tile() +
  geom_text(aes(label = Count), color = "white", size = 5) +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  scale_y_discrete(limits = rev(levels(confusion_df$Actual))) +
  labs(
    title = "Confusion Matrix",
    x = "Predicted Labels",
    y = "Actual Labels",
    fill = "Count"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.text.y = element_text(angle = 45)
  )
```

```{r, echo=FALSE}
tidy_summary <- tidy(multi_log_model)
tidy_summary <- tidy_summary[order(tidy_summary$p.value), ]

# Keep only the rows with the 8 smallest p-values
tidy_top_8 <- head(tidy_summary, 8)

# Print the result
print(tidy_top_8)
```

### Analysis

After conducting the multinomial logistic regression, the model performed with an accuracy of **31.09%** on an unseen test set. Additionally, the confusion matrix above reveals that the model predicted "Data Scientist" **\>1000** times even though it only appears in the test set about **650** times. Meanwhile, it only predicted "Data Analyst" **203** times even though it actually appears in the test set about **300** times. In fact, the confusion matrix reveals that a vast majority of incorrect predictions are when the model predicts "Data Scientist". This weakness in the model is likely due to the fact that the most common job title in the dataset is "Data Scientist", so the model optimized by predicting the most common class.

In the future, we could attempt to remedy the class imbalance issue by resampling from the under-represented classes. However, this is still flawed because without enough data in a class, the trend learned for that class likely won't be generalizeable.

P-values were calculated for each variable at each response level. While these p-values can not be directly interpreted because of a lack of normality in the explanatory variable distributions, they were used to rank the most significant explanatory variables for predicting certain classes. The smallest p-value corresponded to **Experience** as a strong predictor for **Software Engineer** with a coefficient of **0.534**. The second smallest p-value corresponded to **SQL** as a strong predictor against **Research Scientist** with a coefficient of **-1.868**. This means that if an individual is skilled in SQL, their likelihood of being a Research Scientist significantly decreases.

### Conclusion

Ultimately, this model is not effective at predicting what type of data-related job someone will hold based on factors like skills, experience, and education. This conclusion was reached because of the model's low accuracy on the test set as well as significant class imbalances apparent from the confusion matrix.
