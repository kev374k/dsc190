```{r, echo = FALSE}
data <- read.table("gauge.txt", header=TRUE)
head(data, 5)
```

# Question 2: Transform Data and Fit

Based on the theoretical exponential relationship between density and gain, we applied a natural logarithm transformation to gain and looked for a linear relationship.

```{r, echo = FALSE}
par(mfrow = c(1, 2))

data$log_gain <- log(data$gain)

log_model <- lm(log_gain ~ density, data=data)

plot(data$density, data$log_gain,
    main = "ln(Gain) vs Density", 
    xlab = "Snow Density", 
    ylab = "Natural Log of Gain", 
    pch = 19, 
    col = "#89ABE3")

abline(log_model, col = "#EA738D", lwd = 2)
predicted_log_gain <- predict(log_model)
segments(data$density, data$log_gain, data$density, predicted_log_gain, col = "purple", lwd = 1, lty = 2)

residuals <- resid(log_model)
fitted_values <- fitted(log_model)

plot(fitted_values, residuals, 
     main = "Residual Plot", 
     xlab = "Fitted Values", 
     ylab = "Residuals", 
     pch = 19, 
     col = "#89ABE3")

# Add a horizontal line at zero to indicate no residuals
abline(h = 0, col = "#EA738D", lwd = 2, lty = 2)
```

```{r, echo = FALSE}
cat("Logarithm Regression Results")
summary(log_model)
```

We compare the above regression results to the regression results before applying the transformation.

```{r, echo = FALSE}
cat("Regression Results Before Transformation")
summary(model)
```

According to physics, the relationship between density and gain is exponential, not linear. Transforming the data with a natural logarithm reveals a linear relationship because the theoretical relationship is $$g = Ae^{Bd}$$, which transforms into $$ln(g) = ln(A) + Bd$$, establishing a theoretical linear relationship between density and the logarithm of gain.

According to the results of the regression, the logarithm transformation greatly improves the strength of correlation detected. After performing the logarithm, the $$R^2$$ increased from **0.816** to **0.996**. This means 99.6% of the variance in the logarithm of gain can be explained by variance in the snow density. Looking at the actual values of residuals, they are in the range of **-0.15 and 0.2**, compared to the pre-transformation values of **-80 and 130**. This also shows a massive improvement in the regression after applying the logarithm transformation.

The shape of the residual plot still has a very slight parabolic shape but it is much less clear than the shape of the pre-transformation residual plot. Additionally, the dramatic decrease in residual values resulting from the transformation is evidence for its validity.

# Question 3: Effect of Reporting Errors

In order to study the effects of rounding errors, we draw simulated errors from a normal distribution and test different standard deviations for the error distribution to see how it affects the MSE of regression on the simulated erroneous data. Below, the histograms can be seen for when the distribution of density reporting error has a standard deviation of **0.008**, **0.015**, and **0.023**.

```{r, echo = FALSE}
set.seed(49)

n_sims <- 500
n <- nrow(data)

observed_mse <- mean(resid(log_model)^2)
small_error_sim_mse <- numeric(n_sims)

for (i in 1:n_sims) {
  sim_density <- data$density + rnorm(n, mean=0, sd=0.008)
  sim_data <- data.frame(log_gain=predicted_log_gain, density=sim_density)
  sim_model <- lm(log_gain ~ density, data=sim_data)
  small_error_sim_mse[i] = mean(resid(sim_model)^2)
}

hist(small_error_sim_mse, breaks = 30, col = "skyblue", border = "white",
     main = "Histogram of Simulated Regression MSEs\nAverage Reporting Error = 0.008",
     xlab = "MSE", xlim = range(c(small_error_sim_mse, observed_mse)))
abline(v = observed_mse, col = "red", lwd = 2, lty = 2)
legend("topright", legend = c("Actual Regression's MSE"), col = c("red"), lwd = 2, lty = 2)

sim_mse <- numeric(n_sims)

for (i in 1:n_sims) {
  sim_density <- data$density + rnorm(n, mean=0, sd=0.015)
  sim_data <- data.frame(log_gain=predicted_log_gain, density=sim_density)
  sim_model <- lm(log_gain ~ density, data=sim_data)
  sim_mse[i] = mean(resid(sim_model)^2)
}

hist(sim_mse, breaks = 30, col = "skyblue", border = "white",
     main = "Histogram of Simulated Regression MSEs\nAverage Reporting Error = 0.015",
     xlab = "MSE", xlim = range(c(sim_mse, observed_mse)))
abline(v = observed_mse, col = "red", lwd = 2, lty = 2)
legend("topright", legend = c("Actual Regression's MSE"), col = c("red"), lwd = 2, lty = 2)

large_error_sim_mse <- numeric(n_sims)

for (i in 1:n_sims) {
  sim_density <- data$density + rnorm(n, mean=0, sd=0.023)
  sim_data <- data.frame(log_gain=predicted_log_gain, density=sim_density)
  sim_model <- lm(log_gain ~ density, data=sim_data)
  large_error_sim_mse[i] = mean(resid(sim_model)^2)
}

hist(large_error_sim_mse, breaks = 30, col = "skyblue", border = "white",
     main = "Histogram of Simulated Regression MSEs\nAverage Reporting Error = 0.023",
     xlab = "MSE", xlim = range(c(large_error_sim_mse, observed_mse)))
abline(v = observed_mse, col = "red", lwd = 2, lty = 2)
legend("topright", legend = c("Actual Regression's MSE"), col = c("red"), lwd = 2, lty = 2)
```

![](images/clipboard-9133593.png)

The above visualizations show how the MSE of regression responds to different levels of density reporting error. In context, an average reporting error of **0.015** means that the standard deviation of the distribution of error between measured and true density is **0.015**. The mean of this distribution would be 0 because on average, the measurements should approximate the true density.

Under the assumption that density reporting errors come from a normal distribution centered around 0 with a standard deviation of **0.015**, the visualization makes it clear that our observed MSE from the actual regression is not unusual. However, under the assumption that the errors come from a normal distribution with a standard deviation of **0.008** or **0.023**, it would **not** be likely to observe the MSE of our regression on the actual data. This could imply that the true amount of reporting error is closer to **0.015** than 0.008 or 0.023. However, it is worth noting that no conclusive statements can be made about the actual reporting error because other factors could be causing the variation in predicted and actual gains.

We can see from the three visualizations above that as the average reporting error increases, the resulting MSEs increase meaning the regression fit is weaker. This makes sense because more noise/error in measurements would lead to a worse regression result.

# Question 6: Cross-Validate Density Estimation

```{r, echo = FALSE}
cross_val_data <- data[data$density != 0.508, ]

cv_model <- lm(log_gain ~ density, data = cross_val_data)
cv_beta_0 <- coef(cv_model)[1]
cv_beta_1 <- coef(cv_model)[2]
cv_predict_density <- function(gain_value) {
    ln_gain <- log(gain_value)
    density_value <- (ln_gain - cv_beta_0) / cv_beta_1
    return(density_value)
}

pred_densities <- sapply(cross_val_data$gain, cv_predict_density)
density_estimates <- data.frame(
  pred_density <- pred_densities,
  true_density <- cross_val_data$density,
  gain <- cross_val_data$gain,
)

pred_error <- sd(density_estimates$true_density - density_estimates$pred_density)

```
